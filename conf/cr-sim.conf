[DEFAULT]
# total simulation time, in hours, default is 10 years = 87600 hours
total_time = 87600

# raw data which stored in the system, in PB
total_active_storage = 1

# rack amount can be computed by all these data. Chunk size in MBs.
# If not given, datacenter amount will be 1.
chunk_size = 256 
chunks_per_disk = 6000
disks_per_machine = 10
machines_per_rack = 5
# datacenters = 1

# sss, pss, copyset, hdfs, drc, ...
data_placement = sss

data_redundancy = RS_14_10

# if event_file is not empty, events generated will be printed to file
event_file = /root/CR-SIM/log/event

# bandwidth_efficient_scheme = false
# bandwidth in MB/hr
recovery_bandwidth_gap = 25000000 
installment_size = 1000
availability_counts_for_recovery = true

# Output contents
outputs = DL,UNA,RB

# 'true' means use lazy recovery during data recovery.
lazy_recovery = true
# 'true' means use RAFI recovery during data recovery.
rafi_recovery = true

# 'true' means there are more than one data redundancy schemes in the system, 'false' means only one
heterogeneous_redundancy = false
# 'true' means heterogeneous redundancy in all layers 
heterogeneous_medium = false

system_scaling = true
system_upgrade = true
correlated_failures = true
block_failure = true

[Heterogeneous Redundancy]
first_redundancy = RS_3_1
second_redundancy = RS_14_10

# size: replication for small files, erasure codes for big files
# hotness: replication for hot files, erasure codes for cold files
# hb: maintains 'rep + ec' at the same time
hr_policy = hotness

file_size_distribution = /root/CR-SIM/file_size_distribution
hotness_prob = 0.08

[Heterogeneous Medium]
mediums = SSD,HDD
# data redundancy for each layer, format: [first_redundancy, second_redundancy],
# colon as the separator for different layers
# redundancies = (RS_3_1,RS_n_k) : (RS_n_k,LRC_n_k_l)
redundancies = (RS_3_1,RS_14_10) : (RS_14_10,LRC_16_10)
transform_policy = hotness

[Lazy Recovery]
recovery_threshold = 13
max_degraded_slices = 0.1
availability_to_durability_threshold = 0,1,10000
recovery_probability = 0,0

[RAFI Recovery]
# detect interval in hours
detect_interval1 = 0.15
detect_interval2 = 0.15


[System Scaling 1]
style = 0

# in GBs, when style != 0, don't need new_chunks_per_disk
new_chunks_per_disk = 9000

disk_failure_generator = weibull_0.99_10098_1.25
disk_recovery_generator = weibull_1.01_18909_1.53

scaling_start = 5000
inc_capacity = 0.2
# added slices
inc_slices = 30000
add_slice_start = 100
# slices per hour
slice_rate = 10

# 'true' means DSS will balance the load after system scaling, 'false' means not!
load_balancing = false

[System Scaling 2]
style = 1
scaling_start = 15000
inc_capacity = 0.5
inc_slices = 30000
add_slice_start = 100
slice_rate = 20
load_balancing = true

# Rolling update
[System Upgrade 1]
upgrade_start = 1000
# how many machines for each upgrade operator?
upgrade_concurrence = 2
# time interval between two upgrade operators
upgrade_interval = 12
# downtime in hours
downtime = 0.5

# Sample for power outage
[Correlated Failures 1]
occurrence_timestamp = 500
una_scope = 1_rack
una_downtime = 20.0
dl_scope = 1_disk
dl_downtime = 20.5
# "True" means data lost compoenents choose from unavailable components.
choose_from_una = True

# Sample for network breakdown
[Correlated Failures 2]
occurrence_timestamp = 8000
una_scope = 3_rack_3_machine
una_downtime = 0.5

# Sample for schedule outage
[Correlated Failures 3]
occurrence_timestamp = 18000
una_scope = 30_machine
una_downtime = 0.25
dl_scope = 28_disk
dl_downtime = 1

[Block Failure]
block_failure_prob = 0.0001

